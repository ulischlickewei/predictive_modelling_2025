{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f06cbf6",
   "metadata": {},
   "source": [
    "# Detecting Fake News with logistic regression\n",
    "\n",
    "## üì∞ Introduction\n",
    "In this assignment, you will analyze a dataset of news articles labeled as either 'real' or 'fake'.\n",
    "You will explore the data, engineer features, build a logistic regression model package, and evaluate the model's performance.\n",
    "\n",
    "Note: This assignment is based on Chapter 21 of the \"Learning Data Science\" textbook.\n",
    "Link: https://learningds.org/ch/21/fake_news_intro.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e04c041",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93d6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3b84f-7213-4e84-ad2c-7eb6db059408",
   "metadata": {},
   "source": [
    "## üìÅ Step 2: Load and Understand the Data\n",
    "\n",
    "Run the cell below to:\n",
    "- Load the dataset from `fake_news.csv`.\n",
    "- Display the first five rows to get an overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11efaf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fake_news.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9328df-8ca8-49f9-9c8e-b1036e9fc24e",
   "metadata": {},
   "source": [
    "The dataset comprises news articles labeled as either \"REAL\" or \"FAKE\". Each entry includes metadata and content of the article. More specifically, the following variables are included:\n",
    "- `timestamp`: The date and time the article was published or collected.\n",
    "- `baseurl`: The domain or website where the article was published.\n",
    "- `content`: The full text of the news article.\n",
    "- `label`: Indicates whether the article is real or fake. Values are \"REAL\" and \"FAKE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e206764-10c7-4e46-b922-27c2f0c34d1a",
   "metadata": {},
   "source": [
    "### Task 1: missing values\n",
    "Check for data types and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa4a525-a9f0-4bb5-8a37-8813305826b0",
   "metadata": {},
   "source": [
    "*Interpretation*: Your interpretation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b838b-ec47-489f-a649-13c446ca8877",
   "metadata": {},
   "source": [
    "### Task 2: bias\n",
    "(see Section 21.1 of \"Learning Data Science\")\n",
    "\n",
    "This dataset is a simplified version of the FakeNewsNet data repository described in [Shu et al](https://arxiv.org/abs/1809.01286). This repository contains content from news and social media websites, as well as metadata like user engagement metrics. For simplicity, we only look at the dataset‚Äôs political news articles. This subset of the data includes only articles that were fact-checked by Politifact, a nonpartisan organization with a good reputation. Each article in the dataset has a ‚Äúreal‚Äù or ‚Äúfake‚Äù label based on Politifact‚Äôs evaluation, which we use as the ground truth.\n",
    "\n",
    "Politifact uses a nonrandom sampling method to select articles to fact-check. According to its website, Politifact‚Äôs journalists select the ‚Äúmost newsworthy and significant‚Äù claims each day. Politifact started in 2007 and the repository was published in 2020, so most of the articles were published between 2007 and 2020.\n",
    "\n",
    "Summarizing this information, we determine that the target population consists of all political news stories published online in the time period from 2007 to 2020 (we would also want to list the sources of the stories). The access frame is determined by Politifact‚Äôs identification of the most newsworthy claims of the day.\n",
    "\n",
    "Based on the dataset's structure and content, discuss potential sources of bias that could affect model performance. Consider aspects such as:\n",
    "- The origin of the articles (e.g., specific publishers or websites).\n",
    "- The time frame during which the articles were published.\n",
    "- The topics covered and their distribution across real and fake news.\n",
    "- Any preprocessing steps already applied to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067ca10-a0c8-4d5c-b59d-1c4387e94caa",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6a5f44",
   "metadata": {},
   "source": [
    "## üìä Step 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "### Splitting the data in Training and Testing Sets\n",
    "\n",
    "Before diving into EDA, we split our dataset into training (75%) and testing (25%) sets. For this sake, we use the method [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from the Scikit-learn package (click on the function name to access the documentation). Additionally, we ecode our `label` column by using `1` for the `fake` label and `0` for the `real` label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a5434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df['label'] = (df['label'] == 'fake').astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[['timestamp', 'baseurl', 'content']], df['label'],\n",
    "    test_size=0.25, random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53759ff-5048-4a77-afaf-20a083d085d0",
   "metadata": {},
   "source": [
    "### Task 3: Exploring the distribution of real vs fake\n",
    "- Count the number of real vs. fake articles in the training data. (Hint: Use [`value_counts()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html)))\n",
    "- Compute the proportion of fake articles.\n",
    "- Plot the class distribution. (Hint: Use [`sns.histplot()`](https://seaborn.pydata.org/generated/seaborn.histplot.html) or [`plt.hist()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29ae436-9275-44ed-bb9d-e5a00a70b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808a340b-fc58-4d6a-9454-ba49a2727346",
   "metadata": {},
   "source": [
    "### Task 4: Exploring the publishers\n",
    "Analyze the `baseurl` column to inspect article sources:\n",
    "\n",
    "a) Count the number of articles per source. *(Hint: use `value_counts()`; [pandas docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html))*\n",
    "\n",
    "b) Visualize the top 10 sources using a horizontal bar chart with the number of articles as value on the x-axis. (Hint: [`pd.Series.plot(kind='barh')`](https://pandas.pydata.org/docs/reference/api/pandas.Series.plot.html))\n",
    "\n",
    "c) List the top 10 sources of fake news (`label == 1`).\n",
    "\n",
    "d) List the top 10 sources of real news (`label == 0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dac3e9-755a-44fb-ace6-08283b5fee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for a) here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798bf5b-c366-441a-a0ca-61f267a0ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for b) here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64ab19-fbc6-4cd5-83c9-8530a1fe13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for c) here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a23044a-e80a-4580-b07f-b29d11a8704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for d) here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8512e2d-5eef-4efb-95e7-c110ac827dea",
   "metadata": {},
   "source": [
    "*Your observations here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b936bf-b21d-4ab3-a4a9-9b837d13c4d0",
   "metadata": {},
   "source": [
    "### Task 5: Exploring words\n",
    "In this task, we want to explore whether there‚Äôs a connection between the language used in the articles and whether they were identified as fake. A straightforward approach is to focus on specific words‚Äîlike military‚Äîand count how often articles containing that word were labeled as fake. For a word like military to be considered informative, the percentage of fake articles that mention it should be significantly higher or lower than 45%, which is the overall proportion of fake articles in the dataset (264 out of 584).\n",
    "\n",
    "Define a function `make_word_features()` with the following signature:\n",
    "- arguments:\n",
    "    - DataFrame `df` which needs to have a column called `content`\n",
    "    - list `words`\n",
    "- output: DataFrame with the same number of observations as `df` and with one column per word in the input list `word`. For each sample word the new feature contains either `True` or `False` depending on whether the word is contained in `content` or not.\n",
    "\n",
    "\n",
    "*Hints:* \n",
    "- You can create a DataFrame from a dictionary (see [here](https://www.geeksforgeeks.org/how-to-create-dataframe-from-dictionary-in-python-pandas/))\n",
    "- To check whether a certain word in included in a string, use the function [`str.contains()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.contains.html) (click to open the documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a1a84a-8615-436f-8fd0-4074d069ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c43279-e0d7-41ff-8747-250135e10e36",
   "metadata": {},
   "source": [
    "Next, we test your new function with some words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c872462c-0552-4cb5-86d0-0a38d10918a2",
   "metadata": {},
   "source": [
    "### Task 6: Interpreting word predictors\n",
    "In the cell below, we test your new function with some test words. The subsequent cell creates a graph to visualize the results. Carefully look at the test words and at the result of this analysis. What conclusions can we draw from this regarding our modeling task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2343510-9895-489d-a354-9d47869d0cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "word_features = [\n",
    "    'trump', 'clinton', # names of presidents\n",
    "    'state', 'vote', 'congress', 'shutdown', # congress words\n",
    "    'military', 'princ', 'investig', 'antifa', 'joke', 'homeless',\n",
    "    'swamp', 'cnn', 'the' #other possibly useful words\n",
    "]\n",
    "df_words = make_word_features(df, word_features)\n",
    "df_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29285c70-a803-4e65-b534-9d96cee476c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run this cell\n",
    "fake_props = (make_word_features(X_train, word_features)\n",
    " .assign(label=(y_train == 1))\n",
    " .melt(id_vars=['label'], var_name='word', value_name='appeared')\n",
    " .query('appeared == True')\n",
    " .groupby('word')\n",
    " ['label']\n",
    " .agg(['mean', 'count'])\n",
    " .rename(columns={'mean': 'prop_fake'})\n",
    " .sort_values('prop_fake', ascending=False)\n",
    " .reset_index()\n",
    " .melt(id_vars='word')\n",
    ")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=fake_props,\n",
    "    x='value',\n",
    "    y='word',\n",
    "    col='variable',\n",
    "    hue='variable',        # Color-code by the type of metric (prop_fake or count)\n",
    "    s=15,                  # Increase dot size\n",
    "    jitter=False,\n",
    "    sharex=False,\n",
    "    height=3,\n",
    "    aspect=1.3\n",
    ")\n",
    "\n",
    "\n",
    "[[prop_ax, _]] = g.axes\n",
    "prop_ax.axvline(fake_proportion, linestyle='--')\n",
    "prop_ax.set(xlim=(-0.05, 1.05))\n",
    "\n",
    "titles = ['Proportion of articles marked fake', 'Number of articles with word']\n",
    "\n",
    "for ax, title in zip(g.axes.flat, titles):\n",
    "    # Set a different title for each axes\n",
    "    ax.set(title=title)\n",
    "    ax.set(xlabel=None)\n",
    "    ax.set(ylabel=None)\n",
    "    ax.yaxis.grid(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63de86e2-7a07-4de9-a933-36a8d495b938",
   "metadata": {},
   "source": [
    "*Your observations here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6069f-4d0f-4c23-8f61-a1c889c09940",
   "metadata": {},
   "source": [
    "## üß† Step 4: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574acb11-afd5-4d0a-99e6-df2eb7c5c148",
   "metadata": {},
   "source": [
    "### Task 7: Building our first logistic regression model\n",
    "Our EDA showed that the word *vote* is related to whether an article is labeled real or fake. To test this, we fit a logistic regression model using a single binary feature: `1` if the word vote appears in the article and `0` if not.\n",
    "\n",
    "To do so, follow the steps below (see slide 7 of the small slide deck for today's lecture):\n",
    "\n",
    "a) Write a function `lowercase` that takes a dataframe `df` as argument and returns a copy of this dataframe with the `content` column of the `df` dataframe being transformed to lowercase strings. (Hint: apply the [.str.lower()](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.lower.html) method)\n",
    "\n",
    "b) Create a pipeline using [make_pipeline()](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) which includes the following elements:\n",
    "- Preprocessing step 1: your `lowercase` function from step 1 (wrapped into the pipeline using [FunctionTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html))\n",
    "- Preprocessing step 2: your `make_word_features` function from Taks 5, using *vote* as the only word to check for.\n",
    "- Modeling step: `LogisticRegression(penalty='none')\n",
    "\n",
    "c) Train the model on the training data (`fit()`)\n",
    "\n",
    "d) Evaluate the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969721f-fe01-435e-8762-205fe7fe9760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for a) here\n",
    "model1 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203c1f1-d716-44e7-8080-f0d70207634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for b) here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76835af-e246-4c75-9675-70b0757e6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for c) here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c5d3c0-ef0f-4566-83d4-744abde7bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for d) here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9712c3-e6dc-495a-907f-9938d21f0bf3",
   "metadata": {},
   "source": [
    "Run the cell below to see the accuary matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdee8b1-48ae-47ff-90e4-15a6b21ff109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    model1, X_test, y_test, cmap=plt.cm.Blues, colorbar=False\n",
    ")\n",
    "plt.grid(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8897a3c-2a7d-4c32-8681-cce7ac6248a3",
   "metadata": {},
   "source": [
    "### Task 8: Interpreting our first logistic regression model\n",
    "Extract the coefficients of your logistic regression model from Task 7. With these, compute the probability for your model gives to an article that contains the word *vote* to be fake. Do the same for an article that does not contain the word *vote*.\n",
    "\n",
    "(Hint: You can access the logistic regression model from the pipeline by accessing `model1.steps[2][1]` (because the logistic regression is the third step in the pipeline which has the Python index 2, the model is then the second entry of a tuple which has index 1). From this you get the coefficients as `coef_` respectively as `incercept_`. E.g., the *vote* coefficient is computed as `model1.steps[2][1].coef_`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ab0fe-efea-45b6-9a07-76b0eca13bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c923a2d-2976-4a40-96d7-49034d998e74",
   "metadata": {},
   "source": [
    "### Task 9: Building a more complex model\n",
    "Now create a model that uses all of the words we examined in our EDA of the train set, except for *the*. Again compute the accuracy and display the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc7f54a-412c-4276-bfc4-c9ee84622057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "model2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c838196-6c5b-4610-bb03-91234e713f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy: your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e51ec7-fd4a-48e2-9ab1-0ad3383caf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell for the confusion matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    model2, X_test, y_test, cmap=plt.cm.Blues, colorbar=False\n",
    ")\n",
    "plt.grid(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4c37a-ca83-4edb-b234-d9721d9bd89a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
